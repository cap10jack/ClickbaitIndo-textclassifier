{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"newsheadline-scraper.ipynb","provenance":[],"mount_file_id":"1ckR2IfXigAI1F1S45Qw6vnLAXoBLSunu","authorship_tag":"ABX9TyNBarOlJAiJsv+PAd9NP255"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Jh1F2iyQnEWj","colab_type":"code","colab":{}},"source":["import requests\n","from bs4 import BeautifulSoup\n","import numpy as np\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ji_3ggQnMqm","colab_type":"code","colab":{}},"source":["def get_news_detik():\n","    \n","    # url definition\n","    url = \"https://www.jpnn.com/entertainment\"\n","    \n","    # Request\n","    r1 = requests.get(url)\n","    r1.status_code\n","\n","    # We'll save in coverpage the cover page content\n","    coverpage = r1.content\n","\n","    # Soup creation\n","    soup1 = BeautifulSoup(coverpage, 'html5lib')\n","\n","    # News identification\n","    coverpage_news = soup1.find_all(\"h3\") + soup1.find_all(\"h2\") + soup1.find_all(\"h1\") + soup1.find_all(\"li\")\n","    \n","    \n","    number_of_articles = len(coverpage_news)\n","\n","    \n","    list_titles = []\n","    list_links = []\n","\n","    for n in np.arange(0, number_of_articles):\n","\n","       \n","\n","        # Getting the title\n","        title = coverpage_news[n].get_text().strip()\n","        list_titles.append(title)\n","        #link = coverpage_news[n].find('a')['href']\n","        #list_links.append(link)\n","        \n","\n","    df_title=pd.DataFrame(list_titles)\n","    \n","    return (df_title)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVPUXcq32y-1","colab_type":"code","colab":{}},"source":["def get_news_tribun():\n","    \n","    # url definition\n","    url = \"https://www.tribunnews.com/\"\n","    \n","    # Request\n","    r1 = requests.get(url)\n","    r1.status_code\n","\n","    # We'll save in coverpage the cover page content\n","    coverpage = r1.content\n","\n","    # Soup creation\n","    soup1 = BeautifulSoup(coverpage, 'html5lib')\n","\n","    # News identification\n","    coverpage_news = soup1.find_all([\"h4\",\"h3\"])\n","    #len(coverpage_news)\n","    \n","    number_of_articles = len(coverpage_news)\n","\n","  \n","    list_titles = []\n","    \n","    \n","\n","    for n in np.arange(0, number_of_articles):\n","\n","      \n","\n","        # Getting the title\n","        title = coverpage_news[n].find('a').get_text().strip()\n","        list_titles.append(title)\n","        \n","\n","      \n","    df_title=pd.DataFrame(list_titles)\n","    return (df_title)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_botUfPl92tK","colab_type":"code","colab":{}},"source":["def get_news_republika():\n","    \n","    # url definition\n","    url = \"https://republika.co.id/\"\n","    \n","    # Request\n","    r1 = requests.get(url)\n","    r1.status_code\n","\n","    # We'll save in coverpage the cover page content\n","    coverpage = r1.content\n","\n","    # Soup creation\n","    soup1 = BeautifulSoup(coverpage)\n","\n","    # News identification\n","    coverpage_news = soup1.find_all(\"h2\")\n","    #len(coverpage_news)\n","    \n","    number_of_articles = len(coverpage_news)\n","\n","  \n","    list_titles = []\n","    \n","    \n","\n","    for n in np.arange(0, number_of_articles):\n","\n","      \n","\n","        # Getting the title\n","        title = coverpage_news[n].get_text().strip()\n","        list_titles.append(title)\n","        \n","\n","      \n","    df_title=pd.DataFrame(list_titles)\n","    \n","    return (df_title)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rJ-w2hPHepa","colab_type":"code","colab":{}},"source":["def get_news_kapanlagi():\n","    \n","    # url definition\n","    url = \"https://www.kapanlagi.com/\"\n","    \n","    # Request\n","    r1 = requests.get(url)\n","    r1.status_code\n","\n","    # We'll save in coverpage the cover page content\n","    coverpage = r1.content\n","\n","    # Soup creation\n","    soup1 = BeautifulSoup(coverpage)\n","\n","    # News identification\n","    coverpage_news = soup1.find_all(class_='deskrip-berita')\n","    #len(coverpage_news)\n","    \n","    number_of_articles = len(coverpage_news)\n","\n","  \n","    list_titles = []\n","    \n","    \n","\n","    for n in np.arange(0, number_of_articles):\n","\n","      \n","\n","        # Getting the title\n","        title = coverpage_news[n].find('a').get_text().strip()\n","        list_titles.append(title)\n","        \n","\n","      \n","    df_title=pd.DataFrame(list_titles)\n","    \n","    return (df_title)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Py4RjTRaI9TY","colab_type":"code","colab":{}},"source":["def get_news_kompas():\n","    \n","    # url definition\n","    url = \"https://www.kompas.com/\"\n","    \n","    # Request\n","    r1 = requests.get(url)\n","    r1.status_code\n","\n","    # We'll save in coverpage the cover page content\n","    coverpage = r1.content\n","\n","    # Soup creation\n","    soup1 = BeautifulSoup(coverpage)\n","\n","    # News identification\n","    coverpage_news = soup1.find_all(['h2','h3','h4','h5'])\n","    #len(coverpage_news)\n","    \n","    number_of_articles = len(coverpage_news)\n","\n","  \n","    list_titles = []\n","    \n","    \n","\n","    for n in np.arange(0, number_of_articles):\n","\n","      \n","\n","        # Getting the title\n","        title = coverpage_news[n].get_text().strip()\n","        list_titles.append(title)\n","        \n","\n","      \n","    df_title=pd.DataFrame(list_titles)\n","    \n","    return (df_title)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OJmRzvuOKSad","colab_type":"code","colab":{}},"source":["def get_news_tempo():\n","    \n","    # url definition\n","    url = \"https://www.tempo.co/\"\n","    \n","    # Request\n","    r1 = requests.get(url)\n","    r1.status_code\n","\n","    # We'll save in coverpage the cover page content\n","    coverpage = r1.content\n","\n","    # Soup creation\n","    soup1 = BeautifulSoup(coverpage)\n","\n","    # News identification\n","    coverpage_news = soup1.find_all('h2')\n","    #len(coverpage_news)\n","    \n","    number_of_articles = len(coverpage_news)\n","\n","  \n","    list_titles = []\n","    \n","    \n","\n","    for n in np.arange(0, number_of_articles):\n","\n","      \n","\n","        # Getting the title\n","        title = coverpage_news[n].get_text().strip()\n","        list_titles.append(title)\n","        \n","\n","      \n","    df_title=pd.DataFrame(list_titles)\n","    \n","    return (df_title)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G57V45fbL90u","colab_type":"code","colab":{}},"source":["def get_news_oke():\n","    \n","    # url definition\n","    url = \"https://www.okezone.com/\"\n","    \n","    # Request\n","    r1 = requests.get(url)\n","    r1.status_code\n","\n","    # We'll save in coverpage the cover page content\n","    coverpage = r1.content\n","\n","    # Soup creation\n","    soup1 = BeautifulSoup(coverpage)\n","\n","    # News identification\n","    coverpage_news = soup1.find_all('div', class_='mh-title-wrap') + soup1.find_all('div', class_='jdl-right-headline') + soup1.find_all('h3') + soup1.find_all('div', class_='wr-text')\n","    #len(coverpage_news)\n","    \n","    number_of_articles = len(coverpage_news)\n","\n","  \n","    list_titles = []\n","    \n","    \n","\n","    for n in np.arange(0, number_of_articles):\n","\n","      \n","\n","        # Getting the title\n","        title = coverpage_news[n].get_text().strip()\n","        list_titles.append(title)\n","        \n","\n","      \n","    df_title=pd.DataFrame(list_titles)\n","    \n","    return (df_title)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HCuLFBXVP17M","colab_type":"code","colab":{}},"source":["def get_news_fimela():\n","    \n","    # url definition\n","    url = \"https://www.fimela.com/\"\n","    \n","    # Request\n","    r1 = requests.get(url)\n","    r1.status_code\n","\n","    # We'll save in coverpage the cover page content\n","    coverpage = r1.content\n","\n","    # Soup creation\n","    soup1 = BeautifulSoup(coverpage)\n","\n","    # News identification\n","    coverpage_news = soup1.find_all('p') + soup1.find_all('div', class_=\"fimela--articles--snippet__aside\") \n","    #len(coverpage_news)\n","    \n","    number_of_articles = len(coverpage_news)\n","\n","  \n","    list_titles = []\n","    \n","    \n","\n","    for n in np.arange(0, number_of_articles):\n","\n","      \n","\n","        # Getting the title\n","        title = coverpage_news[n].get_text().strip()\n","        list_titles.append(title)\n","        \n","\n","      \n","    df_title=pd.DataFrame(list_titles)\n","    \n","    return (df_title)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YXK0Lre_Rdjq","colab_type":"code","colab":{}},"source":["def get_news_liputan():\n","    \n","    # url definition\n","    url = \"https://www.liputan6.com/\"\n","    \n","    # Request\n","    r1 = requests.get(url)\n","    r1.status_code\n","\n","    # We'll save in coverpage the cover page content\n","    coverpage = r1.content\n","\n","    # Soup creation\n","    soup1 = BeautifulSoup(coverpage)\n","\n","    # News identification\n","    coverpage_news = soup1.find_all(['p','h5','h2','span']) \n","    #len(coverpage_news)\n","    \n","    number_of_articles = len(coverpage_news)\n","\n","  \n","    list_titles = []\n","    \n","    \n","\n","    for n in np.arange(0, number_of_articles):\n","\n","      \n","\n","        # Getting the title\n","        title = coverpage_news[n].get_text().strip()\n","        list_titles.append(title)\n","        \n","\n","      \n","    df_title=pd.DataFrame(list_titles)\n","    \n","    return (df_title)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VvQMq3DaSSr4","colab_type":"code","colab":{}},"source":["def get_news_idn():\n","    \n","    # url definition\n","    url = \"https://www.idntimes.com/\"\n","    \n","    # Request\n","    r1 = requests.get(url)\n","    r1.status_code\n","\n","    # We'll save in coverpage the cover page content\n","    coverpage = r1.content\n","\n","    # Soup creation\n","    soup1 = BeautifulSoup(coverpage)\n","\n","    # News identification\n","    coverpage_news = soup1.find_all('h2', class_=\"title-text\") \n","    #len(coverpage_news)\n","    \n","    number_of_articles = len(coverpage_news)\n","\n","  \n","    list_titles = []\n","    \n","    \n","\n","    for n in np.arange(0, number_of_articles):\n","\n","      \n","\n","        # Getting the title\n","        title = coverpage_news[n].get_text().strip()\n","        list_titles.append(title)\n","        \n","\n","      \n","    df_title=pd.DataFrame(list_titles)\n","    \n","    return (df_title)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tDRqN084T3k7","colab_type":"code","colab":{}},"source":["def get_news_cnn():\n","    \n","    # url definition\n","    url = \"https://www.cnnindonesia.com/\"\n","    \n","    # Request\n","    r1 = requests.get(url)\n","    r1.status_code\n","\n","    # We'll save in coverpage the cover page content\n","    coverpage = r1.content\n","\n","    # Soup creation\n","    soup1 = BeautifulSoup(coverpage)\n","\n","    # News identification\n","    coverpage_news = soup1.find_all(class_=\"title\") \n","    #len(coverpage_news)\n","    \n","    number_of_articles = len(coverpage_news)\n","\n","  \n","    list_titles = []\n","    \n","    \n","\n","    for n in np.arange(0, number_of_articles):\n","\n","      \n","\n","        # Getting the title\n","        title = coverpage_news[n].get_text().strip()\n","        list_titles.append(title)\n","        \n","\n","      \n","    df_title=pd.DataFrame(list_titles)\n","    \n","    return (df_title)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZiNXJv5vAQD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":260},"outputId":"4eac95d7-921f-4f7c-d4df-7527c7b966b7","executionInfo":{"status":"ok","timestamp":1590811514392,"user_tz":-420,"elapsed":1168,"user":{"displayName":"Muhammad Noor Fakhruzzaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEqxeFrYq3MJE6lnwDwIYbszOoi-m2QCCUpGdh=s64","userId":"02619362736302575899"}}},"source":["df_detik = get_news_detik()\n","print(df_detik)\n","#df_detik.to_csv('/content/drive/My Drive/clickbait-textclassifier/jpnn2.csv')\n"],"execution_count":37,"outputs":[{"output_type":"stream","text":["                                                     0\n","0                                             Nasional\n","1                                              Politik\n","2                                               Daerah\n","3                                        Entertainment\n","4                                            Teknologi\n","..                                                 ...\n","234  Aktivis Curigai Pemprov Jatim Mau Bikin Suraba...\n","235  Corona Memangsa 1 Keluarga di Madiun, Lihat Us...\n","236  Uang Ratusan Juta Milik Nasabah BRI Raib, Sald...\n","237  3 Berita Artis Terheboh: Isi Pesan Penyebar Vi...\n","238  Gegara Ini, Umi Pipik Ungkit Masa Lalu Kelam M...\n","\n","[239 rows x 1 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3Wcq1bgvvGBO","colab_type":"code","colab":{}},"source":["df_tribun = get_news_tribun()\n","print(df_tribun)\n","#df_tribun.to_csv('/content/drive/My Drive/clickbait-textclassifier/tribun.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tcJtTe6s-HpU","colab_type":"code","colab":{}},"source":["df_republika = get_news_republika()\n","print(df_republika)\n","#df_republika.to_csv('/content/drive/My Drive/clickbait-textclassifier/republika.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-XsZnb53OD_","colab_type":"code","colab":{}},"source":["df_kapanlagi = get_news_kapanlagi()\n","print(df_kapanlagi)\n","#df_kapanlagi.to_csv('/content/drive/My Drive/clickbait-textclassifier/kapanlagi.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"igFGNjW-IJWM","colab_type":"code","colab":{}},"source":["df_kompas = get_news_kompas()\n","print(df_kompas)\n","df_kompas.to_csv('/content/drive/My Drive/clickbait-textclassifier/kompas.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7bzSs64AJPgZ","colab_type":"code","colab":{}},"source":["df_tempo = get_news_tempo()\n","print(df_tempo)\n","#df_tempo.to_csv('/content/drive/My Drive/clickbait-textclassifier/tempo.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9mBP83qK6-d","colab_type":"code","colab":{}},"source":["df_oke = get_news_oke()\n","print(df_oke)\n","#df_oke.to_csv('/content/drive/My Drive/clickbait-textclassifier/oke.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MwyMqUg4OFdv","colab_type":"code","colab":{}},"source":["df_fimela = get_news_fimela()\n","print(df_fimela)\n","#df_fimela.to_csv('/content/drive/My Drive/clickbait-textclassifier/fimela.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jbyg7axWQOxR","colab_type":"code","colab":{}},"source":["df_liputan = get_news_liputan()\n","print(df_liputan)\n","#df_liputan.to_csv('/content/drive/My Drive/clickbait-textclassifier/liputan6.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jFiYh-ehRvgw","colab_type":"code","colab":{}},"source":["df_idn = get_news_idn()\n","print(df_idn)\n","#df_idn.to_csv('/content/drive/My Drive/clickbait-textclassifier/idntimes.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tkkq8l9kSasl","colab_type":"code","colab":{}},"source":["df_cnn = get_news_cnn()\n","print(df_cnn)\n","#df_cnn.to_csv('/content/drive/My Drive/clickbait-textclassifier/cnnindo.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8yYv5iVyT-JE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}